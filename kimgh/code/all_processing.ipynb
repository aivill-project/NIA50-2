{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 모듈 및 함수\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import open3d as o3d\n",
    "import pickle as pkl\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import glob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def xyxy2xywhn(x, w=1920, h=1200, clip=False, eps=0.0):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    if clip:\n",
    "        clip_boxes(x, (h - eps, w - eps))  # warning: inplace clip\n",
    "    x = np.asarray(x, dtype=float).reshape(1, -1)\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    y = list(y.reshape(-1))\n",
    "    return y\n",
    "\n",
    "def convert_x1y1x2y2_to_tlwh(bbox):\n",
    "    '''\n",
    "    :param bbox: x1 y1 x2 y2\n",
    "    :return: tlwh: top_left x   top_left y    width   height\n",
    "    '''\n",
    "    w = bbox[2] - bbox[0]\n",
    "    h = bbox[3] - bbox[1]\n",
    "    return np.array(([bbox[0], bbox[1], w, h]))\n",
    "\n",
    "# rotation matrix\n",
    "def roty(t, Rx=90/180*np.pi):\n",
    "    ''' Rotation about the y-axis. '''\n",
    "    c = np.cos(t)\n",
    "    s = np.sin(t)\n",
    "    \n",
    "    X = np.array([[1, 0, 0],\n",
    "                    [0, np.cos(Rx), -np.sin(Rx)],\n",
    "                    [0, np.sin(Rx), np.cos(Rx)]])\n",
    "\n",
    "    Z = np.array([[c, -s, 0],\n",
    "                    [s, c, 0],\n",
    "                    [0, 0, 1]])\n",
    "    \n",
    "    return np.matmul(Z, X)\n",
    "\n",
    "def xyz2xyxy(x, y, z, l, w, h, rot_y, extrinsic, intrinsic):\n",
    "    R = roty(rot_y)\n",
    "   \n",
    "    x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2];\n",
    "    y_corners = [h / 2, h / 2, h / 2, h / 2, -h / 2, -h / 2, -h / 2, -h / 2];\n",
    "    z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2];\n",
    "    \n",
    "    corners_3d = np.dot(R, np.vstack([x_corners, y_corners, z_corners]))\n",
    "    corners_3d[0, :] = corners_3d[0, :] + x  # x\n",
    "    corners_3d[1, :] = corners_3d[1, :] + y  # y\n",
    "    corners_3d[2, :] = corners_3d[2, :] + z  # z\n",
    "    corners_3d = np.vstack([corners_3d, [1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "    \n",
    "    point2d = np.matmul(intrinsic, np.matmul(extrinsic, corners_3d))\n",
    "    pointx = np.around(point2d/point2d[2])[0]\n",
    "    pointy = np.around(point2d/point2d[2])[1]\n",
    "\n",
    "    return min(pointx), min(pointy), max(pointx), max(pointy)\n",
    "\n",
    "# 문자열 숫자리스트로 바꾸는 함수\n",
    "def str2list(txt):\n",
    "    txt = txt.replace('\\n', '').split(',')\n",
    "    txt = list(map(float, txt))\n",
    "    \n",
    "    return txt\n",
    "\n",
    "# 리스트를 문자열로 바꾸는 함수\n",
    "def list2str(list):\n",
    "    list = ' '.join(map(str, list))\n",
    "    \n",
    "    return list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 데이터 프레임 만들기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '/data/NIA50/50-2/final/raw'\n",
    "dst = '/data/NIA50/50-2/final/'\n",
    "\n",
    "\n",
    "# Z축 이동을 위해서 calib와 매칭하여 이동범위 지정\n",
    "calib_ls = []\n",
    "# scenes = []\n",
    "calibs = sorted(glob.glob(f'{src}/*/calib/camera/camera_0.json'))\n",
    "for calib in calibs:\n",
    "    scene = re.findall('[a-zA-Z0-9_]+', calib)[-5]\n",
    "    with open(calib, 'r') as f:\n",
    "        calib = json.load(f)\n",
    "    if calib['extrinsic'] not in calib_ls:\n",
    "        calib_ls.append(calib['extrinsic'])\n",
    "        # scenes.append(scene)    \n",
    "calib_typ = {'typ1': {'calib': calib_ls[0], 'mov_zpoint': 14},\n",
    "             'typ2': {'calib': calib_ls[1], 'mov_zpoint': 13},\n",
    "             'typ3': {'calib': calib_ls[2], 'mov_zpoint': 0},\n",
    "             'typ4': {'calib': calib_ls[3], 'mov_zpoint': -20}}\n",
    "\n",
    "\n",
    "# 라벨데이터로 데이터프레임 생성\n",
    "error = []\n",
    "dp_ls = []\n",
    "labels = sorted(glob.glob(f'{src}/*/label/*.json'))\n",
    "for j, label in enumerate(tqdm(labels)):\n",
    "    scene = re.findall('\\w+', label)[-4]\n",
    "    frame = re.findall('\\w+', label)[-2]\n",
    "\n",
    "    # calib값 조정\n",
    "    with open(f'{src}/{scene}/calib/camera/camera_0.json', 'r') as f:\n",
    "        calib = json.load(f)\n",
    "\n",
    "    extrinsic = np.asarray(calib['extrinsic']).reshape(4, 4)\n",
    "    intrinsic = np.zeros([3, 4])\n",
    "    intrinsic[:3, :3] = np.asarray(calib['intrinsic']).reshape(3, 3)\n",
    "    \n",
    "    for typ in ['typ1', 'typ2', 'typ3', 'typ4']:\n",
    "        if calib['extrinsic'] == calib_typ[typ]['calib']:\n",
    "            # extrinsic = np.asarray(calib['extrinsic']).reshape(4, 4)\n",
    "            extrinsic[:3, 3] -= extrinsic[:3, 2] * calib_typ[typ]['mov_zpoint']   \n",
    "\n",
    "    # 컬럼 구성\n",
    "    globals()[f'dp{j}'] = pd.DataFrame()\n",
    "    try:\n",
    "        with open(label, 'r') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        for i in range(len(json_data)):\n",
    "            try:\n",
    "                id_ = json_data[i]['obj_id']\n",
    "                class_ = json_data[i]['obj_type']\n",
    "                psr = json_data[i]['psr']\n",
    "                point_x, point_y, point_z = psr['position']['x'], psr['position']['y'], psr['position']['z']\n",
    "                # z값 범위를 줄이기 위해 조정\n",
    "                for typ in ['typ1', 'typ2', 'typ3', 'typ4']:\n",
    "                    if calib['extrinsic'] == calib_typ[typ]['calib']:\n",
    "                        mov_point_z = calib_typ[typ]['mov_zpoint']\n",
    "                        point_z += mov_point_z\n",
    "                l, w, h = psr['scale']['x'], psr['scale']['y'], psr['scale']['z']\n",
    "                rot_y = psr['rotation']['z']\n",
    "                min_x, min_y, max_x, max_y = xyz2xyxy(point_x, point_y, point_z, l, w, h, rot_y, extrinsic, intrinsic)\n",
    "\n",
    "                data = [\n",
    "                    id_, class_, \n",
    "                    min_x, min_y, max_x, max_y,\n",
    "                    point_x, point_y, point_z,\n",
    "                    l, w, h,\n",
    "                    rot_y,\n",
    "                    intrinsic.flatten().tolist(), extrinsic.flatten().tolist(), mov_point_z,\n",
    "                    scene, frame\n",
    "                    ]\n",
    "                \n",
    "                columns = [\n",
    "                    'id', 'class',\n",
    "                    'min_x', 'min_y', 'max_x', 'max_y',\n",
    "                    'point_x', 'point_y', 'point_z',\n",
    "                    'l', 'w', 'h',\n",
    "                    'rot_y',\n",
    "                    'intrinsic', 'extrinsic', 'mov_point_z',\n",
    "                    'scene', 'frame'\n",
    "                    ]\n",
    "\n",
    "                frame_data = pd.DataFrame(data=[data], columns=columns)\n",
    "                globals()[f'dp{j}'] = pd.concat([globals()[f'dp{j}'], frame_data], axis=0)\n",
    "            except:\n",
    "                error.append({'객체 라벨링 에러': [scene, frame, id_]})\n",
    "    except:\n",
    "        error.append({'json 파일 오류': [scene, frame]})\n",
    "    \n",
    "    dp_ls.append(globals()[f'dp{j}'])\n",
    "dp = pd.concat(dp_ls).reset_index(drop=True)\n",
    "\n",
    "# dp.loc[dp['min_x'] < 0, 'min_x'] = 0\n",
    "# dp.loc[dp['min_x'] > 1920, 'min_x'] = 1920\n",
    "# dp.loc[dp['max_x'] < 0, 'max_x'] = 0\n",
    "# dp.loc[dp['max_x'] > 1920, 'max_x'] = 1920\n",
    "# dp.loc[dp['min_y'] < 0, 'min_y'] = 0\n",
    "# dp.loc[dp['min_y'] > 1200, 'min_y'] = 1200\n",
    "# dp.loc[dp['max_y'] < 0, 'max_y'] = 0\n",
    "# dp.loc[dp['max_y'] > 1200, 'max_y'] = 1200\n",
    "dp['id'] = dp['id'].apply(pd.to_numeric, errors='coerce')\n",
    "dp = dp.dropna(axis=0)\n",
    "# drop_index = dp.loc[(dp['class']==0)| (dp['class']=='Unknown') | (dp['id'].isnull()) | \n",
    "#                     (dp['min_x']-dp['max_x']==0) | (dp['min_y']-dp['max_y']==0) |\n",
    "#                     (dp['point_y']>0) | (dp['point_y']<-50) | (dp['point_x']<-50) | (dp['point_x']>50)].index\n",
    "# dp = dp.drop(drop_index).reset_index(drop=True)\n",
    "dp['id'] = dp['id'].astype(int)\n",
    "dp.to_csv(f'{dst}/data_info.csv')\n",
    "# dp = pd.read_csv('/data/NIA50/50-2/data/nia50_final/data_info.csv', index_col=0, dtype={'frame':object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 체크\n",
    "no_label = []\n",
    "src = '/data/NIA50/50-2/data/nia50_final/raw'\n",
    "labels = sorted(glob.glob(f'{src}/*/label/*.json'))\n",
    "for j, label in enumerate(tqdm(labels)):\n",
    "    scene = re.findall('\\w+', label)[-4]\n",
    "    frame = re.findall('\\w+', label)[-2]\n",
    "\n",
    "    with open(label, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    if len(json_data)==0:\n",
    "        no_label.append(scene)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for i in dict(Counter(no_label)).items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i in dict(Counter(no_label)).items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l, w, h 범위 확인\n",
    "np.around(dp.groupby('class').quantile(0.99)[['l', 'w', 'h']].values, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # 데이터 프레임 불러오기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = pd.read_csv('/data/NIA50/docker/50-2/data/raw/alldata_info.csv', index_col=0, dtype={'frame':object})\n",
    "dp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp['class'].unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # class 통합 1\n",
    "---\n",
    "- Small_Car, Light_Car, Car -> Car\n",
    "- SUV -> SUV_&_Van\n",
    "- Van -> SUV_&_Van\n",
    "- Small_Truck, Medium_Truck, Large_Truck -> Truck\n",
    "- Mini_Bus, Bus -> Bus\n",
    "- Special_Vehicle -> 앵커 기준으로 SUV_&_Van, Truck, Special_Vehicle로 나누기\n",
    "- Two_Wheeler -> Two_Wheeler\n",
    "- Kickboard, Adult, Kid -> Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp = pd.read_csv('/data/NIA50/50-2/data/nia50_final/data_info1.csv', index_col=0, dtype={'frame':object})\n",
    "\n",
    "# class 통합\n",
    "dp.loc[(dp['class']=='Car') | (dp['class']=='Light_Car') | (dp['class']=='Small_Car'), 'class'] = 'Car'\n",
    "dp.loc[(dp['class']=='SUV') | (dp['class']=='Van'), 'class'] = 'SUV_&_Van'\n",
    "dp.loc[(dp['class']=='Adult') | (dp['class']=='Kid') | (dp['class']=='Kickboard'), 'class'] = 'Person'\n",
    "dp.loc[(dp['class']=='Small_Truck') | (dp['class']=='Medium_Truck') | (dp['class']=='Large_Truck'), 'class'] = 'Truck'\n",
    "dp.loc[(dp['class']=='Mini_Bus') | (dp['class']=='Bus'), 'class'] = 'Bus'\n",
    "\n",
    "qt = dp[['class', 'l', 'w', 'h']].groupby('class').quantile(0.99)\n",
    "dp.loc[(dp['class']=='Special_Vehicle') & (dp['l']<=qt.loc['SUV_&_Van']['l']) & (dp['w']<=qt.loc['SUV_&_Van']['w']) & (dp['h']<=qt.loc['SUV_&_Van']['h']), 'class'] = 'SUV_&_Van'\n",
    "dp.loc[(dp['class']=='Special_Vehicle') & (dp['l']>qt.loc['SUV_&_Van']['l']) & (dp['w']>qt.loc['SUV_&_Van']['w']) & (dp['h']>qt.loc['SUV_&_Van']['h'])\n",
    "        & (dp['l']<=qt.loc['Truck']['l']) & (dp['w']<=qt.loc['Truck']['w']) & (dp['h']<=qt.loc['Truck']['h']), 'class'] = 'Truck'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # class 통합 2\n",
    "---\n",
    "- Small_Car, Light_Car, Car -> Car\n",
    "- SUV -> SUV_&_Van\n",
    "- Van -> SUV_&_Van\n",
    "- Small_Truck, Medium_Truck, Large_Truck -> Truck\n",
    "- Mini_Bus, Bus -> Bus\n",
    "- Special_Vehicle -> 육안으로 확인하여 Car, SUV_&_VAN, Truck, Special_Vehicle로 나누기 (Special_Vehicle은 지게차, 포크레인, 야쿠르트 아주머니)\n",
    "- Two_Wheeler -> Two_Wheeler\n",
    "- Kickboard, Adult, Kid -> Person\n",
    "- l, w, h가 너무 작거나 큰 객체는 라벨에서 삭제 (실행 안함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 823586 entries, 0 to 823585\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   id           823586 non-null  int64  \n",
      " 1   class        823586 non-null  object \n",
      " 2   min_x        823586 non-null  float64\n",
      " 3   min_y        823586 non-null  float64\n",
      " 4   max_x        823586 non-null  float64\n",
      " 5   max_y        823586 non-null  float64\n",
      " 6   point_x      823586 non-null  float64\n",
      " 7   point_y      823586 non-null  float64\n",
      " 8   point_z      823586 non-null  float64\n",
      " 9   l            823586 non-null  float64\n",
      " 10  w            823586 non-null  float64\n",
      " 11  h            823586 non-null  float64\n",
      " 12  rot_y        823586 non-null  float64\n",
      " 13  intrinsic    823586 non-null  object \n",
      " 14  extrinsic    823586 non-null  object \n",
      " 15  mov_point_z  823586 non-null  int64  \n",
      " 16  scene        823586 non-null  object \n",
      " 17  frame        823586 non-null  object \n",
      "dtypes: float64(11), int64(2), object(5)\n",
      "memory usage: 113.1+ MB\n"
     ]
    }
   ],
   "source": [
    "dp = pd.read_csv('/data/NIA50/docker/50-2/data/raw/alldata_info.csv', index_col=0, dtype={'frame':object})\n",
    "\n",
    "# class 통합\n",
    "dp.loc[(dp['class']=='Car') | (dp['class']=='Light_Car') | (dp['class']=='Small_Car'), 'class'] = 'Car'\n",
    "dp.loc[(dp['class']=='SUV') | (dp['class']=='Van'), 'class'] = 'SUV_&_Van'\n",
    "dp.loc[(dp['class']=='Adult') | (dp['class']=='Kid') | (dp['class']=='Kickboard'), 'class'] = 'Person'\n",
    "dp.loc[(dp['class']=='Small_Truck') | (dp['class']=='Medium_Truck') | (dp['class']=='Large_Truck'), 'class'] = 'Truck'\n",
    "dp.loc[(dp['class']=='Mini_Bus') | (dp['class']=='Bus'), 'class'] = 'Bus'\n",
    "\n",
    "# cng_sv = pd.read_csv('/data/NIA50/50-2/data/nia50_final/change_special_vehicle.txt', names=['id', 'scene', 'class', 'sub_class'], sep=', ')\n",
    "cng_sv = pd.read_csv('/data/NIA50/docker/50-2/data/raw/cng_sv.csv')\n",
    "for id_, scene in zip(cng_sv['id'].values, cng_sv['scene'].values):\n",
    "    cng_cls = cng_sv.loc[(cng_sv['id']==id_) & (cng_sv['scene']==scene), 'class'].values[0]\n",
    "    dp.loc[(dp['id']==id_) & (dp['scene']==scene) & (dp['class']=='Special_Vehicle'), 'class'] = cng_cls\n",
    "\n",
    "# 탐지 범위 제한\n",
    "dp.loc[dp['min_x'] < 0, 'min_x'] = 0\n",
    "dp.loc[dp['min_x'] > 1920, 'min_x'] = 1920\n",
    "dp.loc[dp['max_x'] < 0, 'max_x'] = 0\n",
    "dp.loc[dp['max_x'] > 1920, 'max_x'] = 1920\n",
    "dp.loc[dp['min_y'] < 0, 'min_y'] = 0\n",
    "dp.loc[dp['min_y'] > 1200, 'min_y'] = 1200\n",
    "dp.loc[dp['max_y'] < 0, 'max_y'] = 0\n",
    "dp.loc[dp['max_y'] > 1200, 'max_y'] = 1200\n",
    "drop_index = dp.loc[(dp['class']==0)| (dp['class']=='Unknown') | (dp['class']=='label error') | (dp['id'].isnull()) | \n",
    "                    (dp['min_x']-dp['max_x']==0) | (dp['min_y']-dp['max_y']==0) |\n",
    "                    (dp['point_y']>0) | (dp['point_y']<-50) | (dp['point_x']<-50) | (dp['point_x']>50)].index\n",
    "dp = dp.drop(drop_index).reset_index(drop=True)\n",
    "\n",
    "# 극단값 제거\n",
    "# for class_ in dp['class'].unique():\n",
    "#     min_Q = dp.loc[dp['class']==class_][['l', 'w', 'h']].quantile(0.05)\n",
    "#     max_Q = dp.loc[dp['class']==class_][['l', 'w', 'h']].quantile(0.99)\n",
    "#     drop_index = dp.loc[(dp['class']==class_) & \n",
    "#                         ((dp['l']<min_Q[0]) | (dp['w']<min_Q[1]) | (dp['h']<min_Q[2]) | (dp['l']>max_Q[0]) | (dp['w']>max_Q[1]) | (dp['h']>max_Q[2]))].index\n",
    "#     dp = dp.drop(drop_index)\n",
    "\n",
    "dp = dp.reset_index(drop=True)\n",
    "dp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 극단값 제거 (IQR 사용)\n",
    "# # dp_Q = dp.groupby('class').quantile([0.25, 0.75])[['l', 'w', 'h']]\n",
    "# for class_ in dp['class'].unique()[2:4]:\n",
    "#     Q1 = dp.loc[dp['class']==class_][['l', 'w', 'h']].quantile(0.25)\n",
    "#     Q3 = dp.loc[dp['class']==class_][['l', 'w', 'h']].quantile(0.75)\n",
    "#     IQR = Q3 - Q1\n",
    "\n",
    "#     outlier = IQR * 1.5\n",
    "#     min_outlier = Q1 - outlier\n",
    "#     max_outlier = Q3 + outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1100148/774834229.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.quantile is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  dp_Q = dp.groupby('class').quantile([0.99])[['l', 'w', 'h']]\n"
     ]
    }
   ],
   "source": [
    "dp_Q = dp.groupby('class').quantile([0.99])[['l', 'w', 'h']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiIndex([(            'Bus', 0.99),\n",
       "            (            'Car', 0.99),\n",
       "            (         'Person', 0.99),\n",
       "            (      'SUV_&_Van', 0.99),\n",
       "            ('Special_Vehicle', 0.99),\n",
       "            (          'Truck', 0.99),\n",
       "            (    'Two_Wheeler', 0.99)],\n",
       "           names=['class', None])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_Q.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>l</th>\n",
       "      <th>w</th>\n",
       "      <th>h</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bus</th>\n",
       "      <th>0.99</th>\n",
       "      <td>12.701542</td>\n",
       "      <td>3.411996</td>\n",
       "      <td>3.845115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Car</th>\n",
       "      <th>0.99</th>\n",
       "      <td>5.085815</td>\n",
       "      <td>2.113723</td>\n",
       "      <td>1.814172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Person</th>\n",
       "      <th>0.99</th>\n",
       "      <td>1.160830</td>\n",
       "      <td>1.168420</td>\n",
       "      <td>1.901956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SUV_&amp;_Van</th>\n",
       "      <th>0.99</th>\n",
       "      <td>5.198146</td>\n",
       "      <td>2.225879</td>\n",
       "      <td>2.146126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Special_Vehicle</th>\n",
       "      <th>0.99</th>\n",
       "      <td>11.578662</td>\n",
       "      <td>2.860172</td>\n",
       "      <td>3.852859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Truck</th>\n",
       "      <th>0.99</th>\n",
       "      <td>12.781159</td>\n",
       "      <td>3.077279</td>\n",
       "      <td>3.989247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Two_Wheeler</th>\n",
       "      <th>0.99</th>\n",
       "      <td>2.559510</td>\n",
       "      <td>1.120389</td>\n",
       "      <td>1.814850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              l         w         h\n",
       "class                                              \n",
       "Bus             0.99  12.701542  3.411996  3.845115\n",
       "Car             0.99   5.085815  2.113723  1.814172\n",
       "Person          0.99   1.160830  1.168420  1.901956\n",
       "SUV_&_Van       0.99   5.198146  2.225879  2.146126\n",
       "Special_Vehicle 0.99  11.578662  2.860172  3.852859\n",
       "Truck           0.99  12.781159  3.077279  3.989247\n",
       "Two_Wheeler     0.99   2.559510  1.120389  1.814850"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12.7, 3.41, 3.85],\n",
       " [5.09, 2.11, 1.81],\n",
       " [1.16, 1.17, 1.9],\n",
       " [5.2, 2.23, 2.15],\n",
       " [11.58, 2.86, 3.85],\n",
       " [12.78, 3.08, 3.99],\n",
       " [2.56, 1.12, 1.81]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.around(dp_Q.values, 2).tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # yolov5\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:08<00:00,  1.74s/it]\n"
     ]
    }
   ],
   "source": [
    "src = '/data/NIA50/50-2/final/raw'\n",
    "dst = '/data/NIA50/docker/50-2/data/yolov5'\n",
    "train_sv_src = '/data/NIA50/docker/50-2/data/raw/train_sv.txt'\n",
    "\n",
    "os.makedirs(f'{dst}/images/train', exist_ok=True)\n",
    "os.makedirs(f'{dst}/images/val', exist_ok=True)\n",
    "os.makedirs(f'{dst}/images/test', exist_ok=True)\n",
    "os.makedirs(f'{dst}/labels/train', exist_ok=True)\n",
    "os.makedirs(f'{dst}/labels/val', exist_ok=True)\n",
    "os.makedirs(f'{dst}/labels/test', exist_ok=True)\n",
    "\n",
    "# os.makedirs(f'{dst}/labels', exist_ok=True)\n",
    "# os.makedirs(f'{dst}/images', exist_ok=True)\n",
    "# os.makedirs(f'{dst}/ImageSets', exist_ok=True)\n",
    "# os.makedirs(f'{dst}/test_images', exist_ok=True)\n",
    "\n",
    "class_num = {'Car': 0,\n",
    "             'SUV_&_Van': 1,\n",
    "             'Truck': 2,\n",
    "             'Bus': 3,\n",
    "             'Special_Vehicle': 4,\n",
    "             'Two_Wheeler': 5,\n",
    "             'Person': 6}\n",
    "\n",
    "dat_typs = []\n",
    "\n",
    "scenes = dp['scene'].unique()[:5]\n",
    "for scene in tqdm(scenes):\n",
    "    dat_typs.append(re.findall('[a-zA-Z]+_[A-Z]_[A-Z]', scene)[0])\n",
    "\n",
    "    frames = dp.loc[dp['scene']==scene, 'frame'].unique()\n",
    "    for frame in frames:\n",
    "\n",
    "\n",
    "        # make points\n",
    "        frame_data = dp.loc[(dp['scene']==scene) & (dp['frame']==frame)].copy()\n",
    "        frame_data['class_num'] = frame_data['class'].apply(lambda x: class_num[x])\n",
    "        xyxy_ls = frame_data[['min_x', 'min_y', 'max_x', 'max_y']].values\n",
    "        \n",
    "        xywhn_ls = []\n",
    "        for xyxy in xyxy_ls:\n",
    "            xywhn = xyxy2xywhn(xyxy)\n",
    "            xywhn_ls.append(xywhn)\n",
    "        \n",
    "        frame_data[['xn', 'yn', 'wn', 'hn']] = xywhn_ls\n",
    "        frame_data[['class_num', 'xn', 'yn', 'wn', 'hn']].to_csv(f'{dst}/labels/{scene}_{frame}.txt', header=False, index=False, sep=' ')\n",
    "\n",
    "\n",
    "        # make images\n",
    "        image_src = f'{src}/{scene}/camera/camera_0/{frame}.jpg'\n",
    "        image_dst = f'{dst}/images/{scene}_{frame}.jpg'\n",
    "        shutil.copyfile(image_src, image_dst)\n",
    "\n",
    "\n",
    "# make ImageSets (split train, val, test)\n",
    "train_ls = []\n",
    "val_ls = []\n",
    "test_ls = []\n",
    "\n",
    "images = sorted(glob.glob(f'{dst}/images/*.jpg'))\n",
    "# dat_typs = set([re.findall('[a-zA-Z]+_[A-Z]_[A-Z]', scene)[0] for scene in scenes])\n",
    "for dat_typ in sorted(set(dat_typs)):\n",
    "    scenes_typ = [scene for scene in scenes if dat_typ in scene]\n",
    "    \n",
    "    train_val, test = train_test_split(scenes_typ, test_size=1/10, shuffle=False, random_state=44)\n",
    "    train, val = train_test_split(train_val, test_size=1/9, random_state=44)\n",
    "\n",
    "    for j in train:\n",
    "        for image in images:\n",
    "            if j in image:\n",
    "                train_ls.append(image)\n",
    "\n",
    "    for j in val:\n",
    "        for image in images:\n",
    "            if j in image:\n",
    "                val_ls.append(image)\n",
    "\n",
    "    for j in test:\n",
    "        for image in images:\n",
    "            if j in image:\n",
    "                test_ls.append(image)\n",
    "\n",
    "for train_img in train_ls:\n",
    "    train_label = train_img.replace('images', 'labels').replace('.jpg', '.txt')\n",
    "    if os.path.exists(train_img) == True:\n",
    "        shutil.move(train_img, f'{dst}/images/train/')\n",
    "    if os.path.exists(train_label) == True:\n",
    "        shutil.move(train_label, f'{dst}/labels/train/')\n",
    "        # shutil.copy(image, f'{dst}/test_images')\n",
    "for val_img in val_ls:\n",
    "    val_label = val_img.replace('images', 'labels').replace('.jpg', '.txt')\n",
    "    if os.path.exists(val_img) == True:\n",
    "        shutil.move(val_img, f'{dst}/images/val/')\n",
    "    if os.path.exists(val_label) == True:\n",
    "        shutil.move(val_label, f'{dst}/labels/val/')\n",
    "    \n",
    "for test_img in test_ls:\n",
    "    test_label = test_img.replace('images', 'labels').replace('.jpg', '.txt')\n",
    "    if os.path.exists(test_img) == True:\n",
    "        shutil.move(test_img, f'{dst}/images/test/')\n",
    "    if os.path.exists(test_label) == True:\n",
    "        shutil.move(test_label, f'{dst}/labels/test/')\n",
    "\n",
    "\n",
    "# with open(f'{dst}/ImageSets/train.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(sorted(train_ls)))\n",
    "    \n",
    "# with open(f'{dst}/ImageSets/val.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(sorted(val_ls)))\n",
    "\n",
    "# with open(f'{dst}/ImageSets/test.txt', 'w') as f:\n",
    "#     f.write('\\n'.join(sorted(test_ls)))\n",
    "\n",
    "\n",
    "# make test_images\n",
    "# with open(f'{dst}/ImageSets/test.txt', 'r') as f:\n",
    "#     test_images = [j.replace('\\n', '') for j in f.readlines()]\n",
    "\n",
    "# for image in test_images:\n",
    "#     if os.path.exists(image) == True:\n",
    "#         shutil.move(image, f'{dst}/test_images')\n",
    "        # shutil.copy(image, f'{dst}/test_images')\n",
    "\n",
    "\n",
    "# special vehicle scene split\n",
    "with open(train_sv_src, 'r') as f:\n",
    "    train_sv_ls = f.read().splitlines()\n",
    "\n",
    "for train_sv in train_sv_ls:\n",
    "    image_sv_ls = sorted(glob.glob(f'{dst}/images/train/{train_sv}*.jpg'))\n",
    "    for image_sv in image_sv_ls:\n",
    "        if os.path.exists(image_sv) == True:\n",
    "            shutil.move(image_sv, f'{dst}/images/test/')\n",
    "    \n",
    "    label_sv_ls = sorted(glob.glob(f'/{dst}/labels/train/{train_sv}*.txt'))\n",
    "    for label_sv in label_sv_ls:\n",
    "        if os.path.exists(label_sv) == True:\n",
    "            shutil.move(label_sv, f'{dst}/labels/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # pvrcnn\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:10<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "src = '/data/NIA50/50-2/final/raw'\n",
    "dst = '/data/NIA50/docker/50-2/data/pvrcnn'\n",
    "train_sv_src = '/data/NIA50/docker/50-2/data/raw/train_sv.txt'\n",
    "\n",
    "os.makedirs(f'{dst}/labels', exist_ok=True)\n",
    "os.makedirs(f'{dst}/points', exist_ok=True)\n",
    "os.makedirs(f'{dst}/ImageSets', exist_ok=True)\n",
    "\n",
    "# # Z축 이동을 위해서 calib와 매칭하여 이동범위 지정\n",
    "# calib_ls = []\n",
    "# # scenes = []\n",
    "# calibs = sorted(glob.glob(f'{src}/*/calib/camera/camera_0.json'))\n",
    "# for calib in calibs:\n",
    "#     scene = re.findall('[a-zA-Z0-9_]+', calib)[-5]\n",
    "#     with open(calib, 'r') as f:\n",
    "#         calib = json.load(f)\n",
    "#     if calib['extrinsic'] not in calib_ls:\n",
    "#         calib_ls.append(calib['extrinsic'])\n",
    "#         # scenes.append(scene)    \n",
    "# calib_typ = {'typ1': {'calib': calib_ls[0], 'mov_zpoint': 14},\n",
    "#              'typ2': {'calib': calib_ls[1], 'mov_zpoint': 13},\n",
    "#              'typ3': {'calib': calib_ls[2], 'mov_zpoint': 0},\n",
    "#              'typ4': {'calib': calib_ls[3], 'mov_zpoint': -20}}\n",
    "\n",
    "dat_typs = []\n",
    "\n",
    "scenes = dp['scene'].unique()[:5]\n",
    "for scene in tqdm(scenes):\n",
    "    dat_typs.append(re.findall('[a-zA-Z]+_[A-Z]_[A-Z]', scene)[0])\n",
    "\n",
    "    # with open(f'{src}/{scene}/calib/camera/camera_0.json', 'r') as f:\n",
    "    #     calib = json.load(f)\n",
    "\n",
    "    # for typ in ['typ1', 'typ2', 'typ3', 'typ4']:\n",
    "    #     if calib['extrinsic'] == calib_typ[typ]['calib']:\n",
    "    #         mov_zpoint = calib_typ[typ]['mov_zpoint']\n",
    "\n",
    "    frames = dp.loc[dp['scene']==scene, 'frame'].unique()\n",
    "    for frame in frames:\n",
    "\n",
    "\n",
    "        # make labels\n",
    "        frame_data = dp.loc[(dp['scene']==scene) & (dp['frame']==frame)].copy()\n",
    "        frame_data[['point_x', 'point_y', 'point_z', 'l', 'w', 'h', 'rot_y', 'class']].to_csv(f'{dst}/labels/{scene}_{frame}.txt', header=False, index=False, sep=' ')\n",
    "\n",
    "\n",
    "        # make points\n",
    "        point_src = f'{src}/{scene}/lidar/{frame}.pcd'\n",
    "        point_dst = f'{dst}/points/{scene}_{frame}.npy'\n",
    "\n",
    "        pcd = o3d.t.io.read_point_cloud(point_src)\n",
    "        positions = pcd.point.positions.numpy()\n",
    "        intensity = pcd.point.intensity.numpy()\n",
    "        positions[:, 2] += frame_data['mov_point_z'].values[0]\n",
    "\n",
    "        pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "        # pcd 범위 자르기\n",
    "        pcd = pcd[np.where((pcd[:, 0]>=-50) & (pcd[:, 0]<=50) & (pcd[:, 1]<=0) & (pcd[:, 1]>=-50) & (pcd[:, 2]>=-4) & (pcd[:, 2]<=8))]\n",
    "        np.save(point_dst, pcd)\n",
    "\n",
    "\n",
    "# make ImageSets (split train, val, test)\n",
    "train_ls = []\n",
    "val_ls = []\n",
    "test_ls = []\n",
    "\n",
    "points = sorted(glob.glob(f'{dst}/points/*.npy'))\n",
    "for dat_typ in sorted(set(dat_typs)):\n",
    "    # images_typ = [image for image in images if dat_typ in image]\n",
    "    scenes_typ = [scene for scene in scenes if dat_typ in scene]\n",
    "    \n",
    "    train_val, test = train_test_split(scenes_typ, test_size=1/10, shuffle=False, random_state=44)\n",
    "    train, val = train_test_split(train_val, test_size=1/9, random_state=44)\n",
    "\n",
    "    for j in train:\n",
    "        for point in points:\n",
    "            if j in point:\n",
    "                point = re.findall('[a-zA-Z0-9_]+', point)[-2]\n",
    "                train_ls.append(point)\n",
    "\n",
    "    for j in val:\n",
    "        for point in points:\n",
    "            if j in point:\n",
    "                point = re.findall('[a-zA-Z0-9_]+', point)[-2]\n",
    "                val_ls.append(point)\n",
    "\n",
    "    for j in test:\n",
    "        for point in points:\n",
    "            if j in point:\n",
    "                point = re.findall('[a-zA-Z0-9_]+', point)[-2]\n",
    "                test_ls.append(point)\n",
    "\n",
    "# special vehicle scene split\n",
    "with open(train_sv_src, 'r') as f:\n",
    "    train_sv_ls = f.read().splitlines()\n",
    "\n",
    "for train_sv in sorted(train_sv_ls):\n",
    "    for train_f in sorted(train_ls):\n",
    "        if train_sv in train_f:\n",
    "            train_ls.remove(train_f)\n",
    "            test_ls.append(train_f)\n",
    "\n",
    "with open(f'{dst}/ImageSets/train.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(train_ls)))\n",
    "    \n",
    "with open(f'{dst}/ImageSets/val.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(val_ls)))\n",
    "\n",
    "with open(f'{dst}/ImageSets/test.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(test_ls)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # deepfusionmot\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dp.groupby('class').agg(['min', 'max', 'mean']).iloc[:, 15:33].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '/data/NIA50/50-2/data/nia50_final/raw'\n",
    "dst = '/data/NIA50/50-2/data/nia50_final/deepfusionmot_integ_final_testset2'\n",
    "\n",
    "src_2d_result = '/data/NIA50/50-2/result/yolov5_integ_final/val_epoch13_testset2'\n",
    "src_3d_result = '/data/NIA50/50-2/result/pvrcnn_integ_final/val_epoch7_testset2/eval'\n",
    "src_scenes = '/data/NIA50/50-2/data/nia50_final/pvrcnn_integ_final/ImageSets/val.txt'\n",
    "\n",
    "os.makedirs(f'{dst}/2D_yolov5', exist_ok=True)\n",
    "os.makedirs(f'{dst}/3D_pvrcnn', exist_ok=True)\n",
    "os.makedirs(f'{dst}/calib', exist_ok=True)\n",
    "os.makedirs(f'{dst}/image_02', exist_ok=True)\n",
    "\n",
    "\n",
    "# make 3D_pvrcnn dataframe\n",
    "# with open(f'{src_3d_result}/result.pkl', 'rb') as f:\n",
    "#     results = pkl.load(f)\n",
    "\n",
    "# frame_df_ls = []\n",
    "# for i, result in enumerate(tqdm(results, desc='3D_pvrcnn 데이터 프레임 생성 중')):\n",
    "        \n",
    "#     scene_frame = result['frame_id']\n",
    "#     scene_ls = [scene_frame[:-5]] * len(result['name'])\n",
    "#     frame_ls = [scene_frame[-4:]] * len(result['name'])\n",
    "#     class_num_ls = result['pred_labels']\n",
    "#     scores = result['score']\n",
    "#     x_ls = result['boxes_lidar'][:, 0]\n",
    "#     y_ls = result['boxes_lidar'][:, 1]\n",
    "#     z_ls = result['boxes_lidar'][:, 2]\n",
    "#     l_ls = result['boxes_lidar'][:, 3]\n",
    "#     w_ls = result['boxes_lidar'][:, 4]\n",
    "#     h_ls = result['boxes_lidar'][:, 5]\n",
    "#     rot_y_ls = result['boxes_lidar'][:, 6]\n",
    "#     extrinsic = json.loads(dp.loc[dp['scene']==scene_ls[0], 'extrinsic'].values[0])\n",
    "#     extrinsic = np.asarray(extrinsic).reshape(4, 4)\n",
    "#     intrinsic = json.loads(dp.loc[dp['scene']==scene_ls[0], 'intrinsic'].values[0])\n",
    "#     intrinsic = np.asarray(intrinsic).reshape(3, 4)\n",
    "\n",
    "#     x1_ls, y1_ls, x2_ls, y2_ls = [], [], [], []\n",
    "#     for x, y, z, l, w, h, rot_y in zip(x_ls, y_ls, z_ls, l_ls, w_ls, h_ls, rot_y_ls):\n",
    "#         x1, y1, x2, y2 = xyz2xyxy(x, y, z, l, w, h, rot_y, extrinsic, intrinsic)\n",
    "#         x1_ls.append(x1)\n",
    "#         y1_ls.append(y1)\n",
    "#         x2_ls.append(x2)\n",
    "#         y2_ls.append(y2)\n",
    "\n",
    "#     data = {'scene': scene_ls,\n",
    "#             'frame': frame_ls, \n",
    "#             'class_num': class_num_ls, \n",
    "#             'x1': x1_ls, \n",
    "#             'y1': y1_ls, \n",
    "#             'x2': x2_ls, \n",
    "#             'y2': y2_ls, \n",
    "#             'score': scores, \n",
    "#             'h': h_ls, \n",
    "#             'w': w_ls, \n",
    "#             'l': l_ls, \n",
    "#             'x': x_ls, \n",
    "#             'y': y_ls, \n",
    "#             'z': z_ls, \n",
    "#             'rot_y': rot_y_ls}\n",
    "#     globals()[f'frame_df{i}'] = pd.DataFrame(data=data)\n",
    "#     frame_df_ls.append(globals()[f'frame_df{i}'])\n",
    "# result_df = pd.concat(frame_df_ls)\n",
    "# result_df['frame'] = result_df['frame'].astype(int)\n",
    "# result_df['alpha'] = 0\n",
    "# result_df.loc[result_df['x1'] < 0, 'x1'] = 0\n",
    "# result_df.loc[result_df['x1'] > 1920, 'x1'] = 1920\n",
    "# result_df.loc[result_df['x2'] < 0, 'x2'] = 0\n",
    "# result_df.loc[result_df['x2'] > 1920, 'x2'] = 1920\n",
    "# result_df.loc[result_df['y1'] < 0, 'y1'] = 0\n",
    "# result_df.loc[result_df['y1'] > 1200, 'y1'] = 1200\n",
    "# result_df.loc[result_df['y2'] < 0, 'y2'] = 0\n",
    "# result_df.loc[result_df['y2'] > 1200, 'y2'] = 1200\n",
    "# result_df = result_df.loc[(result_df['x1']-result_df['x2']!=0) & (result_df['y1']-result_df['y2']!=0)]\n",
    "\n",
    "\n",
    "# test scenes 불러오기\n",
    "with open(src_scenes, 'r') as f:\n",
    "    scenes = [re.sub('\\n', '', i)[:-5] for i in f.readlines()]\n",
    "    scenes = sorted(list(set(scenes)))\n",
    "for scene in tqdm(scenes, desc='deepfusionmot 데이터 생성 중'):\n",
    "\n",
    "\n",
    "    # # make calib\n",
    "    # extrinsic = json.loads(dp.loc[dp['scene']==scene, 'extrinsic'].values[0])[:12]\n",
    "    # intrinsic = json.loads(dp.loc[dp['scene']==scene, 'intrinsic'].values[0])\n",
    "    # # kitti label 형태로 변환\n",
    "    # p2 = intrinsic\n",
    "    # R0_rect = np.eye(3).reshape(-1).tolist()\n",
    "    # Tr_velo_to_cam = extrinsic\n",
    "    # Tr_imu_to_velo = np.zeros((12)).tolist()\n",
    "\n",
    "    # calib_kitti =  ['P0: '+list2str(p2), \n",
    "    #                 'P1: '+list2str(p2), \n",
    "    #                 'P2: '+list2str(p2), \n",
    "    #                 'P3: '+list2str(p2), \n",
    "    #                 'R0_rect: '+list2str(R0_rect), \n",
    "    #                 'Tr_velo_to_cam: '+list2str(Tr_velo_to_cam), \n",
    "    #                 'Tr_imu_to_velo: '+list2str(Tr_imu_to_velo)]\n",
    "\n",
    "    # with open(f'{dst}/calib/{scene}.txt', 'w') as f:\n",
    "    #     f.write('\\n'.join(calib_kitti))\n",
    "\n",
    "\n",
    "    # make 2D_yolov5\n",
    "    w = 1920\n",
    "    h = 1200\n",
    "    labels = sorted(glob.glob(f'{src_2d_result}/labels/{scene}*.txt'))\n",
    "    label_df = pd.DataFrame()\n",
    "    for label in labels:\n",
    "        # with open(label, 'r') as f:\n",
    "        #     bbox = [re.sub('\\n', '', j) for j in f.readlines()]\n",
    "        frame_df = pd.read_csv(label, header=None, sep=' ')\n",
    "        frame_df.columns = ['class', 'x', 'y', 'w', 'h', 'conf']\n",
    "        frame_df['frame'] = int(re.findall('[0-9]+', label)[-1])\n",
    "        frame_df['class'] += 1\n",
    "        frame_df['xmin'] = (frame_df['x'] - frame_df['w']/2) * w\n",
    "        frame_df['ymin'] = (frame_df['y'] - frame_df['h']/2) * h\n",
    "        frame_df['xmax'] = (frame_df['x'] + frame_df['w']/2) * w\n",
    "        frame_df['ymax'] = (frame_df['y'] + frame_df['h']/2) * h\n",
    "\n",
    "        label_df = pd.concat((label_df, frame_df[['frame', 'class', 'xmin', 'ymin', 'xmax', 'ymax', 'conf']]), axis=0)\n",
    "    label_df[['xmin', 'ymin', 'xmax', 'ymax']] = label_df[['xmin', 'ymin', 'xmax', 'ymax']].apply(lambda x: np.around(x))\n",
    "    label_df.to_csv(f'{dst}/2D_yolov5/{scene}.txt', index=None, header=None, sep=',')\n",
    "\n",
    "\n",
    "    # # make 3D_pvrcnn\n",
    "    # result_df.loc[result_df['scene']==scene].iloc[:, 1:].to_csv(f'{dst}/3D_pvrcnn/{scene}.txt', index=None, header=None, sep=',')\n",
    "\n",
    "    \n",
    "    # # make image_02\n",
    "    # img_src = f'{src}/{scene}/camera/camera_0'\n",
    "    # shutil.copytree(img_src, f'{dst}/image_02/{scene}', dirs_exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # TrackEval\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '/data/NIA50/50-2/result/deepfusionmot_integ_final'\n",
    "dst = '/data/NIA50/50-2/data/nia50_final/trackeval_integ_final'\n",
    "\n",
    "os.makedirs(f'{dst}/gt/label_02', exist_ok=True)\n",
    "os.makedirs(f'{dst}/trackers/label_02', exist_ok=True)\n",
    "\n",
    "scenes = sorted(os.listdir(f'{src}/image'))\n",
    "\n",
    "for scene in tqdm(scenes):\n",
    "\n",
    "    # make gt, evaluate_tracking.seqmap.training\n",
    "    seqmap = [f'{scene} empty 0000 0010' for scene in scenes]\n",
    "    seqmap[0] = re.sub('0000', '0001', seqmap[0])\n",
    "    with open(f'{dst}/gt/evaluate_tracking.seqmap.training', 'w') as f:\n",
    "        f.write('\\n'.join(seqmap))\n",
    "\n",
    "    \n",
    "    # make gt, label_02\n",
    "    scene_df = dp.loc[dp['scene']==scene].copy()\n",
    "    scene_df[['truncated', 'occluded', 'alpha']]= 0\n",
    "    scene_df['frame'] = scene_df['frame'].astype(int)\n",
    "    scene_df[['frame']]\n",
    "    scene_df = scene_df[['frame', 'id', 'class', 'truncated', 'occluded', 'alpha', 'min_x', 'min_y', 'max_x', 'max_y', 'h', 'w', 'l', 'point_x', 'point_y', 'point_z', 'rot_y']]\n",
    "    dropped_duple_idx = scene_df[['frame', 'id']].drop_duplicates().index\n",
    "    scene_df = scene_df.loc[dropped_duple_idx].copy()\n",
    "    scene_df.to_csv(f'{dst}/gt/label_02/{scene}.txt', index=None, header=None, sep=' ')\n",
    "\n",
    "\n",
    "    # make trakers, label_02\n",
    "    if os.path.isfile(f'{src}/data/{scene}.txt')==False:\n",
    "        null_data = '0 0 None 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0'\n",
    "        with open(f'{dst}/trackers/label_02/{scene}.txt', 'w') as f:\n",
    "            f.write(null_data)\n",
    "    else:\n",
    "        shutil.copy(f'{src}/data/{scene}.txt', f'{dst}/trackers/label_02/{scene}.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요약 데이터프레임 만들기\n",
    "# allcat = glob.glob('/data/NIA50/50-2/result/trackeval_integ_2/label_02/*.txt')\n",
    "\n",
    "# allcat_ls = []\n",
    "# for cat in allcat:\n",
    "#     cat_name = re.findall('[a-z_&]+_summary', cat)[0]\n",
    "#     cat_name = re.sub('_summary', '', cat_name)\n",
    "#     cat_df = pd.read_csv(cat, sep=' ')\n",
    "#     cat_df.index = [cat_name]\n",
    "\n",
    "#     allcat_ls.append(cat_df)\n",
    "\n",
    "# allcat_df = pd.concat(allcat_ls, axis=0)\n",
    "# allcat_df = allcat_df.iloc[1:, :][['HOTA', 'MOTA', \n",
    "#                                     'DetA', 'AssA', 'DetRe', 'DetPr', 'LocA', 'OWTA',\n",
    "#                                     'MOTP', 'MODA',\n",
    "#                                     'sMOTA', 'IDF1', 'IDR', 'IDP', 'IDTP', 'IDFN', 'IDFP', 'Dets', 'GT_Dets', 'IDs', 'GT_IDs']]\n",
    "# allcat_df.to_csv(f'/data/NIA50/50-2/result/trackeval_integ_2/allcat_summary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiner310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "746d95b724613cc31ae9ea1c95fce8e51ec3ee7393c1b2a647745f061ae2ccda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
