{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 모듈 및 함수\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import open3d as o3d\n",
    "import pickle as pkl\n",
    "import re\n",
    "import json\n",
    "import shutil\n",
    "import glob\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def xyxy2xywhn(x, w=1920, h=1200, clip=False, eps=0.0):\n",
    "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] normalized where xy1=top-left, xy2=bottom-right\n",
    "    if clip:\n",
    "        clip_boxes(x, (h - eps, w - eps))  # warning: inplace clip\n",
    "    x = np.array(x).reshape(1, -1)\n",
    "    y = np.copy(x)\n",
    "    y[:, 0] = ((x[:, 0] + x[:, 2]) / 2) / w  # x center\n",
    "    y[:, 1] = ((x[:, 1] + x[:, 3]) / 2) / h  # y center\n",
    "    y[:, 2] = (x[:, 2] - x[:, 0]) / w  # width\n",
    "    y[:, 3] = (x[:, 3] - x[:, 1]) / h  # height\n",
    "    y = list(y.reshape(-1))\n",
    "    return y\n",
    "\n",
    "# rotation matrix\n",
    "def roty(t, Rx=90/180*np.pi):\n",
    "    ''' Rotation about the y-axis. '''\n",
    "    c = np.cos(t)\n",
    "    s = np.sin(t)\n",
    "    \n",
    "    X = np.array([[1, 0, 0],\n",
    "                    [0, np.cos(Rx), -np.sin(Rx)],\n",
    "                    [0, np.sin(Rx), np.cos(Rx)]])\n",
    "\n",
    "    Z = np.array([[c, -s, 0],\n",
    "                    [s, c, 0],\n",
    "                    [0, 0, 1]])\n",
    "    \n",
    "    return np.matmul(Z, X)\n",
    "\n",
    "def xyz2xyxy(x, y, z, l, w, h, rot_y, extrinsic, intrinsic):\n",
    "    R = roty(rot_y)\n",
    "   \n",
    "    x_corners = [l / 2, l / 2, -l / 2, -l / 2, l / 2, l / 2, -l / 2, -l / 2];\n",
    "    y_corners = [h / 2, h / 2, h / 2, h / 2, -h / 2, -h / 2, -h / 2, -h / 2];\n",
    "    z_corners = [w / 2, -w / 2, -w / 2, w / 2, w / 2, -w / 2, -w / 2, w / 2];\n",
    "    \n",
    "    corners_3d = np.dot(R, np.vstack([x_corners, y_corners, z_corners]))\n",
    "    corners_3d[0, :] = corners_3d[0, :] + x  # x\n",
    "    corners_3d[1, :] = corners_3d[1, :] + y  # y\n",
    "    corners_3d[2, :] = corners_3d[2, :] + z  # z\n",
    "    corners_3d = np.vstack([corners_3d, [1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "    \n",
    "    point2d = np.matmul(intrinsic, np.matmul(extrinsic, corners_3d))\n",
    "    pointx = np.around(point2d/point2d[2])[0]\n",
    "    pointy = np.around(point2d/point2d[2])[1]\n",
    "\n",
    "    return min(pointx), min(pointy), max(pointx), max(pointy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 데이터 프레임 만들기\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59660/59660 [18:49<00:00, 52.81it/s] \n"
     ]
    }
   ],
   "source": [
    "src = '/data/NIA50/50-2/data/nia50_final/raw'\n",
    "dst = '/data/NIA50/50-2/data/nia50_final'\n",
    "\n",
    "\n",
    "# Z축 이동을 위해서 calib와 매칭하여 이동범위 지정\n",
    "calib_ls = []\n",
    "# scenes = []\n",
    "calibs = sorted(glob.glob(f'{src}/*/calib/camera/camera_0.json'))\n",
    "for calib in calibs:\n",
    "    scene = re.findall('[a-zA-Z0-9_]+', calib)[-5]\n",
    "    with open(calib, 'r') as f:\n",
    "        calib = json.load(f)\n",
    "    if calib['extrinsic'] not in calib_ls:\n",
    "        calib_ls.append(calib['extrinsic'])\n",
    "        # scenes.append(scene)    \n",
    "calib_typ = {'typ1': {'calib': calib_ls[0], 'mov_zpoint': 14},\n",
    "             'typ2': {'calib': calib_ls[1], 'mov_zpoint': 13},\n",
    "             'typ3': {'calib': calib_ls[2], 'mov_zpoint': 0},\n",
    "             'typ4': {'calib': calib_ls[3], 'mov_zpoint': -20}}\n",
    "\n",
    "\n",
    "# 라벨데이터로 데이터프레임 생성\n",
    "dp_ls = []\n",
    "labels = sorted(glob.glob(f'{src}/*/label/*.json'))\n",
    "for j, label in enumerate(tqdm(labels)):\n",
    "    scene = re.findall('\\w+', label)[-4]\n",
    "    frame = re.findall('\\w+', label)[-2]\n",
    "\n",
    "    # calib값 조정\n",
    "    with open(f'{src}/{scene}/calib/camera/camera_0.json', 'r') as f:\n",
    "        calib = json.load(f)\n",
    "\n",
    "    extrinsic = np.asarray(calib['extrinsic']).reshape(4, 4)\n",
    "    intrinsic = np.zeros([3, 4])\n",
    "    intrinsic[:3, :3] = np.asarray(calib['intrinsic']).reshape(3, 3)\n",
    "    \n",
    "    for typ in ['typ1', 'typ2', 'typ3', 'typ4']:\n",
    "        if calib['extrinsic'] == calib_typ[typ]['calib']:\n",
    "            # extrinsic = np.asarray(calib['extrinsic']).reshape(4, 4)\n",
    "            extrinsic[:3, 3] -= extrinsic[:3, 2] * calib_typ[typ]['mov_zpoint']   \n",
    "\n",
    "    # 컬럼 구성\n",
    "    globals()[f'dp{j}'] = pd.DataFrame()\n",
    "    with open(label, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    for i in range(len(json_data)):\n",
    "        try:\n",
    "            id_ = json_data[i]['obj_id']\n",
    "            class_ = json_data[i]['obj_type']\n",
    "            psr = json_data[i]['psr']\n",
    "            point_x, point_y, point_z = psr['position']['x'], psr['position']['y'], psr['position']['z']\n",
    "            # z값 범위를 줄이기 위해 조정\n",
    "            for typ in ['typ1', 'typ2', 'typ3', 'typ4']:\n",
    "                if calib['extrinsic'] == calib_typ[typ]['calib']:\n",
    "                    mov_point_z = calib_typ[typ]['mov_zpoint']\n",
    "                    point_z += mov_point_z\n",
    "            l, w, h = psr['scale']['x'], psr['scale']['y'], psr['scale']['z']\n",
    "            rot_y = psr['rotation']['z']\n",
    "            min_x, min_y, max_x, max_y = xyz2xyxy(point_x, point_y, point_z, l, w, h, rot_y, extrinsic, intrinsic)\n",
    "\n",
    "            data = [\n",
    "                id_, class_, \n",
    "                min_x, min_y, max_x, max_y,\n",
    "                point_x, point_y, point_z,\n",
    "                l, w, h,\n",
    "                rot_y,\n",
    "                intrinsic.flatten().tolist(), extrinsic.flatten().tolist(), mov_point_z,\n",
    "                scene, frame\n",
    "                ]\n",
    "            \n",
    "            columns = [\n",
    "                'id', 'class',\n",
    "                'min_x', 'min_y', 'max_x', 'max_y',\n",
    "                'point_x', 'point_y', 'point_z',\n",
    "                'l', 'w', 'h',\n",
    "                'rot_y',\n",
    "                'intrinsic', 'extrinsic', 'mov_point_z',\n",
    "                'scene', 'frame'\n",
    "                ]\n",
    "\n",
    "            frame_data = pd.DataFrame(data=[data], columns=columns)\n",
    "            globals()[f'dp{j}'] = pd.concat([globals()[f'dp{j}'], frame_data], axis=0)\n",
    "        except:\n",
    "            continue\n",
    "    dp_ls.append(globals()[f'dp{j}'])\n",
    "    \n",
    "dp = pd.concat(dp_ls).reset_index(drop=True)\n",
    "\n",
    "dp.loc[dp['min_x'] < 0, 'min_x'] = 0\n",
    "dp.loc[dp['min_x'] > 1920, 'min_x'] = 1920\n",
    "dp.loc[dp['max_x'] < 0, 'max_x'] = 0\n",
    "dp.loc[dp['max_x'] > 1920, 'max_x'] = 1920\n",
    "dp.loc[dp['min_y'] < 0, 'min_y'] = 0\n",
    "dp.loc[dp['min_y'] > 1200, 'min_y'] = 1200\n",
    "dp.loc[dp['max_y'] < 0, 'max_y'] = 0\n",
    "dp.loc[dp['max_y'] > 1200, 'max_y'] = 1200\n",
    "dp['id'] = dp['id'].apply(pd.to_numeric, errors='coerce')\n",
    "drop_index = dp.loc[(dp['class']==0) | (dp['class']=='Unknown') | (dp['id'].isnull()) | (dp['min_x']-dp['max_x']==0) | (dp['min_y']-dp['max_y']==0)].index\n",
    "dp = dp.drop(drop_index).reset_index(drop=True)\n",
    "dp['id'] = dp['id'].astype(int)\n",
    "dp.to_csv(f'{dst}/data_info.csv')\n",
    "# dp = pd.read_csv('/data/NIA50/50-2/data/nia50_final/data_info.csv', index_col=0, dtype={'frame':object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 체크\n",
    "no_label = []\n",
    "src = '/data/NIA50/50-2/data/nia50_final/raw'\n",
    "labels = sorted(glob.glob(f'{src}/*/label/*.json'))\n",
    "for j, label in enumerate(tqdm(labels)):\n",
    "    scene = re.findall('\\w+', label)[-4]\n",
    "    frame = re.findall('\\w+', label)[-2]\n",
    "\n",
    "    with open(label, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    if len(json_data)==0:\n",
    "        no_label.append(scene)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "for i in dict(Counter(no_label)).items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for i in dict(Counter(no_label)).items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l, w, h 범위 확인\n",
    "np.around(dp.groupby('class').quantile(0.99)[['l', 'w', 'h']].values, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # class 통합 1\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 563424 entries, 0 to 563423\n",
      "Data columns (total 18 columns):\n",
      " #   Column       Non-Null Count   Dtype  \n",
      "---  ------       --------------   -----  \n",
      " 0   id           563424 non-null  int64  \n",
      " 1   class        563424 non-null  object \n",
      " 2   min_x        563424 non-null  float64\n",
      " 3   min_y        563424 non-null  float64\n",
      " 4   max_x        563424 non-null  float64\n",
      " 5   max_y        563424 non-null  float64\n",
      " 6   point_x      563424 non-null  float64\n",
      " 7   point_y      563424 non-null  float64\n",
      " 8   point_z      563424 non-null  float64\n",
      " 9   l            563424 non-null  float64\n",
      " 10  w            563424 non-null  float64\n",
      " 11  h            563424 non-null  float64\n",
      " 12  rot_y        563424 non-null  float64\n",
      " 13  intrinsic    563424 non-null  object \n",
      " 14  extrinsic    563424 non-null  object \n",
      " 15  mov_point_z  563424 non-null  int64  \n",
      " 16  scene        563424 non-null  object \n",
      " 17  frame        563424 non-null  object \n",
      "dtypes: float64(11), int64(2), object(5)\n",
      "memory usage: 81.7+ MB\n"
     ]
    }
   ],
   "source": [
    "dp = pd.read_csv('/data/NIA50/50-2/data/nia50_final/data_info.csv', index_col=0, dtype={'frame':object})\n",
    "dp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class 통합\n",
    "dp.loc[(dp['class']=='Car') | (dp['class']=='Light_Car') | (dp['class']=='Small_Car'), 'class'] = 'Car'\n",
    "dp.loc[(dp['class']=='SUV') | (dp['class']=='Van'), 'class'] = 'SUV_&_Van'\n",
    "dp.loc[(dp['class']=='Adult') | (dp['class']=='Kid') | (dp['class']=='Kickboard'), 'class'] = 'Person'\n",
    "dp.loc[(dp['class']=='Small_Truck') | (dp['class']=='Medium_Truck') | (dp['class']=='Large_Truck'), 'class'] = 'Truck'\n",
    "dp.loc[(dp['class']=='Mini_Bus') | (dp['class']=='Bus'), 'class'] = 'Bus'\n",
    "\n",
    "qt = dp[['class', 'l', 'w', 'h']].groupby('class').quantile(0.99)\n",
    "dp.loc[(dp['class']=='Special_Vehicle') & (dp['l']<=qt.loc['SUV_&_Van']['l']) & (dp['w']<=qt.loc['SUV_&_Van']['w']) & (dp['h']<=qt.loc['SUV_&_Van']['h']), 'class'] = 'SUV_&_Van'\n",
    "dp.loc[(dp['class']=='Special_Vehicle') & (dp['l']>qt.loc['SUV_&_Van']['l']) & (dp['w']>qt.loc['SUV_&_Van']['w']) & (dp['h']>qt.loc['SUV_&_Van']['h'])\n",
    "        & (dp['l']<=qt.loc['Truck']['l']) & (dp['w']<=qt.loc['Truck']['w']) & (dp['h']<=qt.loc['Truck']['h']), 'class'] = 'Truck'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # yolov5\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4886/4886 [1:25:52<00:00,  1.05s/it]  \n"
     ]
    }
   ],
   "source": [
    "src = '/data/NIA50/50-2/data/nia50_final/raw'\n",
    "dst = '/data/NIA50/50-2/data/nia50_final/yolov5_integ'\n",
    "\n",
    "os.makedirs(f'{dst}/labels', exist_ok=True)\n",
    "os.makedirs(f'{dst}/images', exist_ok=True)\n",
    "os.makedirs(f'{dst}/ImageSets', exist_ok=True)\n",
    "os.makedirs(f'{dst}/test_images', exist_ok=True)\n",
    "\n",
    "class_num = {'Car': 0,\n",
    "             'SUV_&_Van': 1,\n",
    "             'Truck': 2,\n",
    "             'Bus': 3,\n",
    "             'Special_Vehicle': 4,\n",
    "             'Two_Wheeler': 5,\n",
    "             'Person': 6}\n",
    "\n",
    "dat_typs = []\n",
    "\n",
    "scenes = dp['scene'].unique()\n",
    "for scene in tqdm(scenes):\n",
    "    dat_typs.append(re.findall('[a-zA-Z]+_[A-Z]_[A-Z]', scene)[0])\n",
    "\n",
    "    frames = dp.loc[dp['scene']==scene, 'frame'].unique()\n",
    "    for frame in frames:\n",
    "\n",
    "        # make points\n",
    "        frame_data = dp.loc[(dp['scene']==scene) & (dp['frame']==frame)].copy()\n",
    "        frame_data['class_num'] = frame_data['class'].apply(lambda x: class_num[x])\n",
    "        xyxy_ls = frame_data[['min_x', 'min_y', 'max_x', 'max_y']].values\n",
    "        \n",
    "        xywhn_ls = []\n",
    "        for xyxy in xyxy_ls:\n",
    "            xywhn = xyxy2xywhn(xyxy)\n",
    "            xywhn_ls.append(xywhn)\n",
    "        \n",
    "        frame_data[['xn', 'yn', 'wn', 'hn']] = xywhn_ls\n",
    "        frame_data[['class_num', 'xn', 'yn', 'wn', 'hn']].to_csv(f'{dst}/labels/{scene}_{frame}.txt', header=False, index=False, sep=' ')\n",
    "\n",
    "        # make images\n",
    "        image_src = f'{src}/{scene}/camera/camera_0/{frame}.jpg'\n",
    "        image_dst = f'{dst}/images/{scene}_{frame}.jpg'\n",
    "        shutil.copyfile(image_src, image_dst)\n",
    "\n",
    "# make ImageSets\n",
    "train_ls = []\n",
    "val_ls = []\n",
    "test_ls = []\n",
    "\n",
    "images = sorted(glob.glob(f'{dst}/images/*.jpg'))\n",
    "# dat_typs = set([re.findall('[a-zA-Z]+_[A-Z]_[A-Z]', scene)[0] for scene in scenes])\n",
    "for dat_typ in sorted(set(dat_typs)):\n",
    "    scenes_typ = [scene for scene in scenes if dat_typ in scene]\n",
    "    \n",
    "    train_val, test = train_test_split(scenes_typ, test_size=0.1, shuffle=False, random_state=44)\n",
    "    train, val = train_test_split(train_val, test_size=0.2, random_state=44)\n",
    "\n",
    "    for j in train:\n",
    "        for image in images:\n",
    "            if j in image:\n",
    "                train_ls.append(image)\n",
    "\n",
    "    for j in val:\n",
    "        for image in images:\n",
    "            if j in image:\n",
    "                val_ls.append(image)\n",
    "\n",
    "    for j in test:\n",
    "        for image in images:\n",
    "            if j in image:\n",
    "                test_ls.append(image)\n",
    "\n",
    "with open(f'{dst}/ImageSets/train.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(train_ls)))\n",
    "    \n",
    "with open(f'{dst}/ImageSets/val.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(val_ls)))\n",
    "\n",
    "with open(f'{dst}/ImageSets/test.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(test_ls)))\n",
    "\n",
    "# make test_images\n",
    "with open(f'{dst}/ImageSets/test.txt', 'r') as f:\n",
    "    test_images = [j.replace('\\n', '') for j in f.readlines()]\n",
    "\n",
    "for image in test_images:\n",
    "    if os.path.exists(image) == True:\n",
    "        shutil.move(image, f'{dst}/test_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # pvrcnn\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 2163/4886 [23:43<29:52,  1.52it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m frames \u001b[39m=\u001b[39m dp\u001b[39m.\u001b[39mloc[dp[\u001b[39m'\u001b[39m\u001b[39mscene\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39mscene, \u001b[39m'\u001b[39m\u001b[39mframe\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39munique()\n\u001b[1;32m     20\u001b[0m \u001b[39mfor\u001b[39;00m frame \u001b[39min\u001b[39;00m frames:\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m     \u001b[39m# make labels\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m     frame_data \u001b[39m=\u001b[39m dp\u001b[39m.\u001b[39mloc[(dp[\u001b[39m'\u001b[39m\u001b[39mscene\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m==\u001b[39mscene) \u001b[39m&\u001b[39m (dp[\u001b[39m'\u001b[39;49m\u001b[39mframe\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m==\u001b[39;49mframe)]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     24\u001b[0m     frame_data[[\u001b[39m'\u001b[39m\u001b[39mpoint_x\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpoint_y\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mpoint_z\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39ml\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mh\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrot_y\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mclass\u001b[39m\u001b[39m'\u001b[39m]]\u001b[39m.\u001b[39mto_csv(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdst\u001b[39m}\u001b[39;00m\u001b[39m/labels/\u001b[39m\u001b[39m{\u001b[39;00mscene\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00mframe\u001b[39m}\u001b[39;00m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m, header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, sep\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spiner310/lib/python3.10/site-packages/pandas/core/ops/common.py:72\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[39mreturn\u001b[39;00m \u001b[39mNotImplemented\u001b[39m\n\u001b[1;32m     70\u001b[0m other \u001b[39m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 72\u001b[0m \u001b[39mreturn\u001b[39;00m method(\u001b[39mself\u001b[39;49m, other)\n",
      "File \u001b[0;32m~/anaconda3/envs/spiner310/lib/python3.10/site-packages/pandas/core/arraylike.py:42\u001b[0m, in \u001b[0;36mOpsMixin.__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39m@unpack_zerodim_and_defer\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m__eq__\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__eq__\u001b[39m(\u001b[39mself\u001b[39m, other):\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cmp_method(other, operator\u001b[39m.\u001b[39;49meq)\n",
      "File \u001b[0;32m~/anaconda3/envs/spiner310/lib/python3.10/site-packages/pandas/core/series.py:6243\u001b[0m, in \u001b[0;36mSeries._cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6240\u001b[0m rvalues \u001b[39m=\u001b[39m extract_array(other, extract_numpy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, extract_range\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m   6242\u001b[0m \u001b[39mwith\u001b[39;00m np\u001b[39m.\u001b[39merrstate(\u001b[39mall\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 6243\u001b[0m     res_values \u001b[39m=\u001b[39m ops\u001b[39m.\u001b[39;49mcomparison_op(lvalues, rvalues, op)\n\u001b[1;32m   6245\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_construct_result(res_values, name\u001b[39m=\u001b[39mres_name)\n",
      "File \u001b[0;32m~/anaconda3/envs/spiner310/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:287\u001b[0m, in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[39mreturn\u001b[39;00m invalid_comparison(lvalues, rvalues, op)\n\u001b[1;32m    286\u001b[0m \u001b[39melif\u001b[39;00m is_object_dtype(lvalues\u001b[39m.\u001b[39mdtype) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(rvalues, \u001b[39mstr\u001b[39m):\n\u001b[0;32m--> 287\u001b[0m     res_values \u001b[39m=\u001b[39m comp_method_OBJECT_ARRAY(op, lvalues, rvalues)\n\u001b[1;32m    289\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     res_values \u001b[39m=\u001b[39m _na_arithmetic_op(lvalues, rvalues, op, is_cmp\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/spiner310/lib/python3.10/site-packages/pandas/core/ops/array_ops.py:75\u001b[0m, in \u001b[0;36mcomp_method_OBJECT_ARRAY\u001b[0;34m(op, x, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39mvec_compare(x\u001b[39m.\u001b[39mravel(), y\u001b[39m.\u001b[39mravel(), op)\n\u001b[1;32m     74\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     result \u001b[39m=\u001b[39m libops\u001b[39m.\u001b[39;49mscalar_compare(x\u001b[39m.\u001b[39;49mravel(), y, op)\n\u001b[1;32m     76\u001b[0m \u001b[39mreturn\u001b[39;00m result\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "src = '/data/NIA50/50-2/data/nia50_final/raw'\n",
    "dst = '/data/NIA50/50-2/data/nia50_final/pvrcnn_integ'\n",
    "\n",
    "os.makedirs(f'{dst}/labels', exist_ok=True)\n",
    "os.makedirs(f'{dst}/points', exist_ok=True)\n",
    "os.makedirs(f'{dst}/ImageSets', exist_ok=True)\n",
    "\n",
    "scenes = dp['scene'].unique()\n",
    "for scene in tqdm(scenes):\n",
    "    # dat_typs.append(re.findall('[a-zA-Z]+_[A-Z]_[A-Z]', scene)[0])\n",
    "\n",
    "    # with open(f'{src}/{scene}/calib/camera/camera_0.json', 'r') as f:\n",
    "    #     calib = json.load(f)\n",
    "\n",
    "    # for typ in ['typ1', 'typ2', 'typ3', 'typ4']:\n",
    "    #     if calib['extrinsic'] == calib_typ[typ]['calib']:\n",
    "    #         mov_zpoint = calib_typ[typ]['mov_zpoint']\n",
    "\n",
    "    frames = dp.loc[dp['scene']==scene, 'frame'].unique()\n",
    "    for frame in frames:\n",
    "\n",
    "        # make labels\n",
    "        frame_data = dp.loc[(dp['scene']==scene) & (dp['frame']==frame)].copy()\n",
    "        frame_data[['point_x', 'point_y', 'point_z', 'l', 'w', 'h', 'rot_y', 'class']].to_csv(f'{dst}/labels/{scene}_{frame}.txt', header=False, index=False, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = '/data/NIA50/50-2/data/nia50_final/raw'\n",
    "dst = '/data/NIA50/50-2/data/nia50_final/pvrcnn_integ'\n",
    "\n",
    "os.makedirs(f'{dst}/labels', exist_ok=True)\n",
    "os.makedirs(f'{dst}/points', exist_ok=True)\n",
    "os.makedirs(f'{dst}/ImageSets', exist_ok=True)\n",
    "\n",
    "# # Z축 이동을 위해서 calib와 매칭하여 이동범위 지정\n",
    "# calib_ls = []\n",
    "# # scenes = []\n",
    "# calibs = sorted(glob.glob(f'{src}/*/calib/camera/camera_0.json'))\n",
    "# for calib in calibs:\n",
    "#     scene = re.findall('[a-zA-Z0-9_]+', calib)[-5]\n",
    "#     with open(calib, 'r') as f:\n",
    "#         calib = json.load(f)\n",
    "#     if calib['extrinsic'] not in calib_ls:\n",
    "#         calib_ls.append(calib['extrinsic'])\n",
    "#         # scenes.append(scene)    \n",
    "# calib_typ = {'typ1': {'calib': calib_ls[0], 'mov_zpoint': 14},\n",
    "#              'typ2': {'calib': calib_ls[1], 'mov_zpoint': 13},\n",
    "#              'typ3': {'calib': calib_ls[2], 'mov_zpoint': 0},\n",
    "#              'typ4': {'calib': calib_ls[3], 'mov_zpoint': -20}}\n",
    "\n",
    "dat_typs = []\n",
    "\n",
    "scenes = dp['scene'].unique()\n",
    "for scene in tqdm(scenes):\n",
    "    dat_typs.append(re.findall('[a-zA-Z]+_[A-Z]_[A-Z]', scene)[0])\n",
    "\n",
    "    # with open(f'{src}/{scene}/calib/camera/camera_0.json', 'r') as f:\n",
    "    #     calib = json.load(f)\n",
    "\n",
    "    # for typ in ['typ1', 'typ2', 'typ3', 'typ4']:\n",
    "    #     if calib['extrinsic'] == calib_typ[typ]['calib']:\n",
    "    #         mov_zpoint = calib_typ[typ]['mov_zpoint']\n",
    "\n",
    "    frames = dp.loc[dp['scene']==scene, 'frame'].unique()\n",
    "    for frame in frames:\n",
    "\n",
    "        # make labels\n",
    "        frame_data = dp.loc[(dp['scene']==scene) & (dp['frame']==frame)].copy()\n",
    "        frame_data[['point_x', 'point_y', 'point_z', 'l', 'w', 'h', 'rot_y', 'class']].to_csv(f'{dst}/labels/{scene}_{frame}.txt', header=False, index=False, sep=' ')\n",
    "\n",
    "        # make points\n",
    "        point_src = f'{src}/{scene}/lidar/{frame}.pcd'\n",
    "        point_dst = f'{dst}/points/{scene}_{frame}.npy'\n",
    "\n",
    "        pcd = o3d.t.io.read_point_cloud(point_src)\n",
    "        positions = pcd.point.positions.numpy()\n",
    "        intensity = pcd.point.intensity.numpy()\n",
    "        positions[:, 2] += frame_data['mov_point_z'].values[0]\n",
    "\n",
    "        pcd = np.concatenate((positions, intensity), axis = 1)\n",
    "        np.save(point_dst, pcd)\n",
    "\n",
    "# make ImageSets\n",
    "train_ls = []\n",
    "val_ls = []\n",
    "test_ls = []\n",
    "\n",
    "points = sorted(glob.glob(f'{dst}/points/*.npy'))\n",
    "for dat_typ in sorted(set(dat_typs)):\n",
    "    # images_typ = [image for image in images if dat_typ in image]\n",
    "    scenes_typ = [scene for scene in scenes if dat_typ in scene]\n",
    "    \n",
    "    train_val, test = train_test_split(scenes_typ, test_size=0.1, shuffle=False, random_state=44)\n",
    "    train, val = train_test_split(train_val, test_size=0.2, random_state=44)\n",
    "\n",
    "    for j in train:\n",
    "        for point in points:\n",
    "            if j in point:\n",
    "                point = re.findall('[a-zA-Z0-9_]+', point)[-2]\n",
    "                train_ls.append(point)\n",
    "\n",
    "    for j in val:\n",
    "        for point in points:\n",
    "            if j in point:\n",
    "                point = re.findall('[a-zA-Z0-9_]+', point)[-2]\n",
    "                val_ls.append(point)\n",
    "\n",
    "    for j in test:\n",
    "        for point in points:\n",
    "            if j in point:\n",
    "                point = re.findall('[a-zA-Z0-9_]+', point)[-2]\n",
    "                test_ls.append(point)\n",
    "\n",
    "with open(f'{dst}/ImageSets/train.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(train_ls)))\n",
    "    \n",
    "with open(f'{dst}/ImageSets/val.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(val_ls)))\n",
    "\n",
    "with open(f'{dst}/ImageSets/test.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sorted(test_ls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## # deepfusionmot\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spiner310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "746d95b724613cc31ae9ea1c95fce8e51ec3ee7393c1b2a647745f061ae2ccda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
